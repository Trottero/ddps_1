<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--
 | Generated by Apache Maven Doxia at 2020-07-06
 | Rendered using Apache Maven Stylus Skin 1.5
-->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Apache Hadoop 3.3.0 &#x2013; Hadoop in Secure Mode</title>
    <style type="text/css" media="all">
      @import url("./css/maven-base.css");
      @import url("./css/maven-theme.css");
      @import url("./css/site.css");
    </style>
    <link rel="stylesheet" href="./css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20200706" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
                </head>
  <body class="composite">
    <div id="banner">
                        <a href="http://hadoop.apache.org/" id="bannerLeft">
                                        <img src="http://hadoop.apache.org/images/hadoop-logo.jpg" alt="" />
                </a>
                              <a href="http://www.apache.org/" id="bannerRight">
                                        <img src="http://www.apache.org/images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                   <div class="xleft">
                          <a href="http://www.apache.org/" class="externalLink">Apache</a>
        &gt;
                  <a href="http://hadoop.apache.org/" class="externalLink">Hadoop</a>
        &gt;
                  <a href="../index.html">Apache Hadoop Project Dist POM</a>
        &gt;
                  <a href="index.html">Apache Hadoop 3.3.0</a>
        &gt;
        Hadoop in Secure Mode
        </div>
            <div class="xright">            <a href="http://wiki.apache.org/hadoop" class="externalLink">Wiki</a>
            |
                <a href="https://gitbox.apache.org/repos/asf/hadoop.git" class="externalLink">git</a>
            |
                <a href="http://hadoop.apache.org/" class="externalLink">Apache Hadoop</a>
              
                                   &nbsp;| Last Published: 2020-07-06
              &nbsp;| Version: 3.3.0
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                   <h5>General</h5>
                  <ul>
                  <li class="none">
                  <a href="../../index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CommandsManual.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/FileSystemShell.html">FileSystem Shell</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Compatibility.html">Compatibility Specification</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/DownstreamDev.html">Downstream Developer's Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/AdminCompatibilityGuide.html">Admin Compatibility Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/InterfaceClassification.html">Interface Classification</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/filesystem/index.html">FileSystem Specification</a>
            </li>
          </ul>
                       <h5>Common</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html">CLI Mini Cluster</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/FairCallQueue.html">Fair Call Queue</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Superusers.html">Proxy User</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/RackAwareness.html">Rack Awareness</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/SecureMode.html">Secure Mode</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/ServiceLevelAuth.html">Service Level Authorization</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/HttpAuthentication.html">HTTP Authentication</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CredentialProviderAPI.html">Credential Provider API</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-kms/index.html">Hadoop KMS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Tracing.html">Tracing</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/UnixShellGuide.html">Unix Shell Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/registry/index.html">Registry</a>
            </li>
          </ul>
                       <h5>HDFS</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">User Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">NameNode HA With QJM</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">NameNode HA With NFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html">Observer NameNode</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/Federation.html">Federation</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ViewFs.html">ViewFs</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">Snapshots</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html">Edits Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html">Image Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">Permissions and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html">Quotas and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/LibHdfs.html">libhdfs (C API)</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS (REST API)</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-hdfs-httpfs/index.html">HttpFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">Short Circuit Local Reads</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">NFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">Rolling Upgrade</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html">Extended Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">Transparent Encryption</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html">Multihoming</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Storage Policies</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/MemoryStorage.html">Memory Storage Support</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/SLGUserGuide.html">Synthetic Load Generator</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html">Erasure Coding</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html">Disk Balancer</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsUpgradeDomain.html">Upgrade Domain</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsDataNodeAdminGuide.html">DataNode Admin</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html">Router Federation</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsProvidedStorage.html">Provided Storage</a>
            </li>
          </ul>
                       <h5>MapReduce</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Tutorial</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">Compatibility with 1.x</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html">Encrypted Shuffle</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html">Pluggable Shuffle/Sort</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html">Distributed Cache Deploy</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/SharedCacheSupport.html">Support for YARN Shared Cache</a>
            </li>
          </ul>
                       <h5>MapReduce REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">MR History Server</a>
            </li>
          </ul>
                       <h5>YARN</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YARN.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YarnCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/FairScheduler.html">Fair Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html">ResourceManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager HA</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceModel.html">Resource Model</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeLabel.html">Node Labels</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeAttributes.html">Node Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html">Timeline Service V.2</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html">YARN Application Security</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeManager.html">NodeManager</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/DockerContainers.html">Running Applications in Docker Containers</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/RuncContainers.html">Running Applications in runC Containers</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html">Using CGroups</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/SecureContainer.html">Secure Containers</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ReservationSystem.html">Reservation System</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/GracefulDecommission.html">Graceful Decommission</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/OpportunisticContainers.html">Opportunistic Containers</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/Federation.html">YARN Federation</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/SharedCache.html">Shared Cache</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/UsingGpus.html">Using GPU</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/UsingFPGA.html">Using FPGA</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/PlacementConstraints.html">Placement Constraints</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YarnUI2.html">YARN UI2</a>
            </li>
          </ul>
                       <h5>YARN REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Introduction</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">Resource Manager</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">Node Manager</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html#Timeline_Server_REST_API_v1">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html#Timeline_Service_v.2_REST_API">Timeline Service V.2</a>
            </li>
          </ul>
                       <h5>YARN Service</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/Overview.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/QuickStart.html">QuickStart</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/Concepts.html">Concepts</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/YarnServiceAPI.html">Yarn Service API</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/ServiceDiscovery.html">Service Discovery</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/SystemServices.html">System Services</a>
            </li>
          </ul>
                       <h5>Hadoop Compatible File Systems</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-aliyun/tools/hadoop-aliyun/index.html">Aliyun OSS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-aws/tools/hadoop-aws/index.html">Amazon S3</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-azure/index.html">Azure Blob Storage</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-azure-datalake/index.html">Azure Data Lake Storage</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-openstack/index.html">OpenStack Swift</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-cos/cloud-storage/index.html">Tencent COS</a>
            </li>
          </ul>
                       <h5>Auth</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-auth/index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/Examples.html">Examples</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/Configuration.html">Configuration</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/BuildingIt.html">Building</a>
            </li>
          </ul>
                       <h5>Tools</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-streaming/HadoopStreaming.html">Hadoop Streaming</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-archives/HadoopArchives.html">Hadoop Archives</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-archive-logs/HadoopArchiveLogs.html">Hadoop Archive Logs</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-distcp/DistCp.html">DistCp</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-gridmix/GridMix.html">GridMix</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-rumen/Rumen.html">Rumen</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-resourceestimator/ResourceEstimator.html">Resource Estimator Service</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-sls/SchedulerLoadSimulator.html">Scheduler Load Simulator</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Benchmarking.html">Hadoop Benchmarking</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-dynamometer/Dynamometer.html">Dynamometer</a>
            </li>
          </ul>
                       <h5>Reference</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/release/">Changelog and Release Notes</a>
            </li>
                  <li class="none">
                  <a href="../../api/index.html">Java API docs</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/UnixShellAPI.html">Unix Shell API</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Metrics.html">Metrics</a>
            </li>
          </ul>
                       <h5>Configuration</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/core-default.xml">core-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs-rbf/hdfs-rbf-default.xml">hdfs-rbf-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/DeprecatedProperties.html">Deprecated Properties</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="./images/logos/maven-feather.png"/>
        </a>
                       
                               </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!---
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<h1>Hadoop in Secure Mode</h1>
<ul>
<li><a href="#Introduction">Introduction</a></li>
<li><a href="#Authentication">Authentication</a>
<ul>
<li><a href="#End_User_Accounts">End User Accounts</a></li>
<li><a href="#User_Accounts_for_Hadoop_Daemons">User Accounts for Hadoop Daemons</a></li>
<li><a href="#Kerberos_principals_for_Hadoop_Daemons">Kerberos principals for Hadoop Daemons</a>
<ul>
<li><a href="#HDFS">HDFS</a></li>
<li><a href="#YARN">YARN</a></li>
<li><a href="#MapReduce_JobHistory_Server">MapReduce JobHistory Server</a></li></ul></li>
<li><a href="#Mapping_from_Kerberos_principals_to_OS_user_accounts">Mapping from Kerberos principals to OS user accounts</a></li>
<li><a href="#Example_rules">Example rules</a></li>
<li><a href="#Mapping_from_user_to_group">Mapping from user to group</a></li>
<li><a href="#Proxy_user">Proxy user</a></li>
<li><a href="#Secure_DataNode">Secure DataNode</a></li></ul></li>
<li><a href="#Data_confidentiality">Data confidentiality</a>
<ul>
<li><a href="#Data_Encryption_on_RPC">Data Encryption on RPC</a></li>
<li><a href="#Data_Encryption_on_Block_data_transfer.">Data Encryption on Block data transfer.</a></li>
<li><a href="#Data_Encryption_on_HTTP">Data Encryption on HTTP</a></li></ul></li>
<li><a href="#Configuration">Configuration</a>
<ul>
<li><a href="#Permissions_for_both_HDFS_and_local_fileSystem_paths">Permissions for both HDFS and local fileSystem paths</a></li>
<li><a href="#Common_Configurations">Common Configurations</a></li>
<li><a href="#NameNode">NameNode</a></li>
<li><a href="#Secondary_NameNode">Secondary NameNode</a></li>
<li><a href="#JournalNode">JournalNode</a></li>
<li><a href="#DataNode">DataNode</a></li>
<li><a href="#WebHDFS">WebHDFS</a></li>
<li><a href="#ResourceManager">ResourceManager</a></li>
<li><a href="#NodeManager">NodeManager</a></li>
<li><a href="#Configuration_for_WebAppProxy">Configuration for WebAppProxy</a></li>
<li><a href="#LinuxContainerExecutor">LinuxContainerExecutor</a></li>
<li><a href="#MapReduce_JobHistory_Server">MapReduce JobHistory Server</a></li></ul></li>
<li><a href="#Multihoming">Multihoming</a></li>
<li><a href="#Troubleshooting">Troubleshooting</a></li>
<li><a href="#Troubleshooting_with_KDiag">Troubleshooting with KDiag</a>
<ul>
<li><a href="#Usage">Usage</a>
<ul>
<li><a href="#a--jaas:_Require_a_JAAS_file_to_be_defined_in_java.security.auth.login.config.">--jaas: Require a JAAS file to be defined in java.security.auth.login.config.</a></li>
<li><a href="#a--keylen_.3Clength.3E:_Require_a_minimum_size_for_encryption_keys_supported_by_the_JVM.22.">--keylen &lt;length&gt;: Require a minimum size for encryption keys supported by the JVM&quot;.</a></li>
<li><a href="#a--keytab_.3Ckeytab.3E_--principal_.3Cprincipal.3E:_Log_in_from_a_keytab.">--keytab &lt;keytab&gt; --principal &lt;principal&gt;: Log in from a keytab.</a></li>
<li><a href="#a--nofail_:_Do_not_fail_on_the_first_problem">--nofail : Do not fail on the first problem</a></li>
<li><a href="#a--nologin:_Do_not_attempt_to_log_in.">--nologin: Do not attempt to log in.</a></li>
<li><a href="#a--out_outfile:_Write_output_to_file.">--out outfile: Write output to file.</a></li>
<li><a href="#a--resource_.3Cresource.3E_:_XML_configuration_resource_to_load.">--resource &lt;resource&gt; : XML configuration resource to load.</a></li>
<li><a href="#a--secure:_Fail_if_the_command_is_not_executed_on_a_secure_cluster.">--secure: Fail if the command is not executed on a secure cluster.</a></li>
<li><a href="#a--verifyshortname_.3Cprincipal.3E:_validate_the_short_name_of_a_principal">--verifyshortname &lt;principal&gt;: validate the short name of a principal</a></li></ul></li>
<li><a href="#Example">Example</a></li></ul></li>
<li><a href="#References">References</a></li></ul>

<div class="section">
<h2><a name="Introduction"></a>Introduction</h2>
<p>This document describes how to configure authentication for Hadoop in secure mode. When Hadoop is configured to run in secure mode, each Hadoop service and each user must be authenticated by Kerberos.</p>
<p>Forward and reverse host lookup for all service hosts must be configured correctly to allow services to authenticate with each other. Host lookups may be configured using either DNS or <tt>/etc/hosts</tt> files. Working knowledge of Kerberos and DNS is recommended before attempting to configure Hadoop services in Secure Mode.</p>
<p>Security features of Hadoop consist of <a href="#Authentication">Authentication</a>, <a href="./ServiceLevelAuth.html">Service Level Authorization</a>, <a href="./HttpAuthentication.html">Authentication for Web Consoles</a> and <a href="#Data_confidentiality">Data Confidentiality</a>.</p></div>
<div class="section">
<h2><a name="Authentication"></a>Authentication</h2>
<div class="section">
<h3><a name="End_User_Accounts"></a>End User Accounts</h3>
<p>When service level authentication is turned on, end users must authenticate themselves before interacting with Hadoop services. The simplest way is for a user to authenticate interactively using the <a class="externalLink" href="http://web.mit.edu/kerberos/krb5-1.12/doc/user/user_commands/kinit.html" title="MIT Kerberos Documentation of kinit">Kerberos <tt>kinit</tt> command</a>. Programmatic authentication using Kerberos keytab files may be used when interactive login with <tt>kinit</tt> is infeasible.</p></div>
<div class="section">
<h3><a name="User_Accounts_for_Hadoop_Daemons"></a>User Accounts for Hadoop Daemons</h3>
<p>Ensure that HDFS and YARN daemons run as different Unix users, e.g. <tt>hdfs</tt> and <tt>yarn</tt>. Also, ensure that the MapReduce JobHistory server runs as different user such as <tt>mapred</tt>.</p>
<p>It&#x2019;s recommended to have them share a Unix group, e.g. <tt>hadoop</tt>. See also &#x201c;<a href="#Mapping_from_user_to_group">Mapping from user to group</a>&#x201d; for group management.</p>
<table border="0" class="bodyTable">
<thead>

<tr class="a">
<th align="left"> User:Group    </th>
<th align="left"> Daemons                                             </th></tr>
</thead><tbody>

<tr class="b">
<td align="left"> hdfs:hadoop   </td>
<td align="left"> NameNode, Secondary NameNode, JournalNode, DataNode </td></tr>
<tr class="a">
<td align="left"> yarn:hadoop   </td>
<td align="left"> ResourceManager, NodeManager                        </td></tr>
<tr class="b">
<td align="left"> mapred:hadoop </td>
<td align="left"> MapReduce JobHistory Server                         </td></tr>
</tbody>
</table></div>
<div class="section">
<h3><a name="Kerberos_principals_for_Hadoop_Daemons"></a>Kerberos principals for Hadoop Daemons</h3>
<p>Each Hadoop Service instance must be configured with its Kerberos principal and keytab file location.</p>
<p>The general format of a Service principal is <tt>ServiceName/_HOST@REALM.TLD</tt>. e.g. <tt>dn/_HOST@EXAMPLE.COM</tt>.</p>
<p>Hadoop simplifies the deployment of configuration files by allowing the hostname component of the service principal to be specified as the <tt>_HOST</tt> wildcard. Each service instance will substitute <tt>_HOST</tt> with its own fully qualified hostname at runtime. This allows administrators to deploy the same set of configuration files on all nodes. However, the keytab files will be different.</p>
<div class="section">
<h4><a name="HDFS"></a>HDFS</h4>
<p>The NameNode keytab file, on each NameNode host, should look like the following:</p>

<div>
<div>
<pre class="source">$ klist -e -k -t /etc/security/keytab/nn.service.keytab
Keytab name: FILE:/etc/security/keytab/nn.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
</pre></div></div>

<p>The Secondary NameNode keytab file, on that host, should look like the following:</p>

<div>
<div>
<pre class="source">$ klist -e -k -t /etc/security/keytab/sn.service.keytab
Keytab name: FILE:/etc/security/keytab/sn.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
</pre></div></div>

<p>The DataNode keytab file, on each host, should look like the following:</p>

<div>
<div>
<pre class="source">$ klist -e -k -t /etc/security/keytab/dn.service.keytab
Keytab name: FILE:/etc/security/keytab/dn.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
</pre></div></div>
</div>
<div class="section">
<h4><a name="YARN"></a>YARN</h4>
<p>The ResourceManager keytab file, on the ResourceManager host, should look like the following:</p>

<div>
<div>
<pre class="source">$ klist -e -k -t /etc/security/keytab/rm.service.keytab
Keytab name: FILE:/etc/security/keytab/rm.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
</pre></div></div>

<p>The NodeManager keytab file, on each host, should look like the following:</p>

<div>
<div>
<pre class="source">$ klist -e -k -t /etc/security/keytab/nm.service.keytab
Keytab name: FILE:/etc/security/keytab/nm.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
</pre></div></div>
</div>
<div class="section">
<h4><a name="MapReduce_JobHistory_Server"></a>MapReduce JobHistory Server</h4>
<p>The MapReduce JobHistory Server keytab file, on that host, should look like the following:</p>

<div>
<div>
<pre class="source">$ klist -e -k -t /etc/security/keytab/jhs.service.keytab
Keytab name: FILE:/etc/security/keytab/jhs.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
</pre></div></div>
</div></div>
<div class="section">
<h3><a name="Mapping_from_Kerberos_principals_to_OS_user_accounts"></a>Mapping from Kerberos principals to OS user accounts</h3>
<p>Hadoop maps Kerberos principals to OS user (system) accounts using rules specified by <tt>hadoop.security.auth_to_local</tt>. How Hadoop evaluates these rules is determined by the setting of <tt>hadoop.security.auth_to_local.mechanism</tt>.</p>
<p>In the default <tt>hadoop</tt> mode a Kerberos principal <i>must</i> be matched against a rule that transforms the principal to a simple form, i.e. a user account name without &#x2018;@&#x2019; or &#x2018;/&#x2019;, otherwise a principal will not be authorized and a error will be logged.  In case of the <tt>MIT</tt> mode the rules work in the same way as the <tt>auth_to_local</tt> in <a class="externalLink" href="http://web.mit.edu/Kerberos/krb5-latest/doc/admin/conf_files/krb5_conf.html">Kerberos configuration file (krb5.conf)</a> and the restrictions of <tt>hadoop</tt> mode do <i>not</i> apply. If you use <tt>MIT</tt> mode it is suggested to use the same <tt>auth_to_local</tt> rules that are specified in your /etc/krb5.conf as part of your default realm and keep them in sync. In both <tt>hadoop</tt> and <tt>MIT</tt> mode the rules are being applied (with the exception of <tt>DEFAULT</tt>) to <i>all</i> principals regardless of their specified realm. Also, note you should <i>not</i> rely on the <tt>auth_to_local</tt> rules as an ACL and use proper (OS) mechanisms.</p>
<p>Possible values for <tt>auth_to_local</tt> are:</p>
<ul>

<li>

<p><tt>RULE:exp</tt> The local name will be formulated from exp. The format for exp is <tt>[n:string](regexp)s/pattern/replacement/g</tt>. The integer n indicates how many components the target principal should have. If this matches, then a string will be formed from string, substituting the realm of the principal for <tt>$0</tt> and the n&#x2019;th component of the principal for <tt>$n</tt> (e.g., if the principal was johndoe/admin then <tt>[2:$2$1foo]</tt> would result in the string <tt>adminjohndoefoo</tt>). If this string matches regexp, then the <tt>s//[g]</tt> substitution command will be run over the string. The optional g will cause the substitution to be global over the string, instead of replacing only the first match in the string. As an extension to MIT, Hadoop <tt>auth_to_local</tt> mapping supports the <b>/L</b> flag that lowercases the returned name.</p>
</li>
<li>

<p><tt>DEFAULT</tt> Picks the first component of the principal name as the system user name if and only if the realm matches the <tt>default_realm</tt> (usually defined in /etc/krb5.conf). e.g. The default rule maps the principal <tt>host/full.qualified.domain.name@MYREALM.TLD</tt> to system user <tt>host</tt> if the default realm is <tt>MYREALM.TLD</tt>.</p>
</li>
</ul>
<p>In case no rules are specified Hadoop defaults to using <tt>DEFAULT</tt>, which is probably <i>not suitable</i> to most of the clusters.</p>
<p>Please note that Hadoop does not support multiple default realms (e.g like Heimdal does). Also, Hadoop does not do a verification on mapping whether a local system account exists.</p></div>
<div class="section">
<h3><a name="Example_rules"></a>Example rules</h3>
<p>In a typical cluster HDFS and YARN services will be launched as the system <tt>hdfs</tt> and <tt>yarn</tt> users respectively. <tt>hadoop.security.auth_to_local</tt> can be configured as follows:</p>

<div>
<div>
<pre class="source">&lt;property&gt;
  &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;
  &lt;value&gt;
    RULE:[2:$1/$2@$0]([ndj]n/.*@REALM.\TLD)s/.*/hdfs/
    RULE:[2:$1/$2@$0]([rn]m/.*@REALM\.TLD)s/.*/yarn/
    RULE:[2:$1/$2@$0](jhs/.*@REALM\.TLD)s/.*/mapred/
    DEFAULT
  &lt;/value&gt;
&lt;/property&gt;
</pre></div></div>

<p>This would map any principal <tt>nn, dn, jn</tt> on any <tt>host</tt> from realm <tt>REALM.TLD</tt> to the local system account <tt>hdfs</tt>. Secondly it would map any principal <tt>rm, nm</tt> on any <tt>host</tt> from <tt>REALM.TLD</tt> to the local system account <tt>yarn</tt>. Thirdly, it would map the principal <tt>jhs</tt> on any <tt>host</tt> from realm <tt>REALM.TLD</tt> to the local system account <tt>mapred</tt>. Finally, any principal on any host from the default realm will be mapped to the user component of that principal.</p>
<p>Custom rules can be tested using the <tt>hadoop kerbname</tt> command.  This command allows one to specify a principal and apply Hadoop&#x2019;s current <tt>auth_to_local</tt> ruleset.</p></div>
<div class="section">
<h3><a name="Mapping_from_user_to_group"></a>Mapping from user to group</h3>
<p>The system user to system group mapping mechanism can be configured via <tt>hadoop.security.group.mapping</tt>. See <a href="GroupsMapping.html">Hadoop Groups Mapping</a> for details.</p>
<p>Practically you need to manage SSO environment using Kerberos with LDAP for Hadoop in secure mode.</p></div>
<div class="section">
<h3><a name="Proxy_user"></a>Proxy user</h3>
<p>Some products such as Apache Oozie which access the services of Hadoop on behalf of end users need to be able to impersonate end users. See <a href="./Superusers.html">the doc of proxy user</a> for details.</p></div>
<div class="section">
<h3><a name="Secure_DataNode"></a>Secure DataNode</h3>
<p>Because the DataNode data transfer protocol does not use the Hadoop RPC framework, DataNodes must authenticate themselves using privileged ports which are specified by <tt>dfs.datanode.address</tt> and <tt>dfs.datanode.http.address</tt>. This authentication is based on the assumption that the attacker won&#x2019;t be able to get root privileges on DataNode hosts.</p>
<p>When you execute the <tt>hdfs datanode</tt> command as root, the server process binds privileged ports at first, then drops privilege and runs as the user account specified by <tt>HDFS_DATANODE_SECURE_USER</tt>. This startup process uses <a class="externalLink" href="https://commons.apache.org/proper/commons-daemon/jsvc.html" title="Link to Apache Commons Jsvc">the jsvc program</a> installed to <tt>JSVC_HOME</tt>. You must specify <tt>HDFS_DATANODE_SECURE_USER</tt> and <tt>JSVC_HOME</tt> as environment variables on start up (in <tt>hadoop-env.sh</tt>).</p>
<p>As of version 2.6.0, SASL can be used to authenticate the data transfer protocol. In this configuration, it is no longer required for secured clusters to start the DataNode as root using <tt>jsvc</tt> and bind to privileged ports. To enable SASL on data transfer protocol, set <tt>dfs.data.transfer.protection</tt> in hdfs-site.xml. A SASL enabled DataNode can be started in secure mode in following two ways: 1. Set a non-privileged port for <tt>dfs.datanode.address</tt>. 1. Set <tt>dfs.http.policy</tt> to <tt>HTTPS_ONLY</tt> or set <tt>dfs.datanode.http.address</tt> to a privileged port and make sure the <tt>HDFS_DATANODE_SECURE_USER</tt> and <tt>JSVC_HOME</tt> environment variables are specified properly as environment variables on start up (in <tt>hadoop-env.sh</tt>).</p>
<p>In order to migrate an existing cluster that used root authentication to start using SASL instead, first ensure that version 2.6.0 or later has been deployed to all cluster nodes as well as any external applications that need to connect to the cluster. Only versions 2.6.0 and later of the HDFS client can connect to a DataNode that uses SASL for authentication of data transfer protocol, so it is vital that all callers have the correct version before migrating. After version 2.6.0 or later has been deployed everywhere, update configuration of any external applications to enable SASL. If an HDFS client is enabled for SASL, then it can connect successfully to a DataNode running with either root authentication or SASL authentication. Changing configuration for all clients guarantees that subsequent configuration changes on DataNodes will not disrupt the applications. Finally, each individual DataNode can be migrated by changing its configuration and restarting. It is acceptable to have a mix of some DataNodes running with root authentication and some DataNodes running with SASL authentication temporarily during this migration period, because an HDFS client enabled for SASL can connect to both.</p></div></div>
<div class="section">
<h2><a name="Data_confidentiality"></a>Data confidentiality</h2>
<div class="section">
<h3><a name="Data_Encryption_on_RPC"></a>Data Encryption on RPC</h3>
<p>The data transfered between hadoop services and clients can be encrypted on the wire. Setting <tt>hadoop.rpc.protection</tt> to <tt>privacy</tt> in <tt>core-site.xml</tt> activates data encryption.</p></div>
<div class="section">
<h3><a name="Data_Encryption_on_Block_data_transfer."></a>Data Encryption on Block data transfer.</h3>
<p>You need to set <tt>dfs.encrypt.data.transfer</tt> to <tt>true</tt> in the hdfs-site.xml in order to activate data encryption for data transfer protocol of DataNode.</p>
<p>Optionally, you may set <tt>dfs.encrypt.data.transfer.algorithm</tt> to either <tt>3des</tt> or <tt>rc4</tt> to choose the specific encryption algorithm. If unspecified, then the configured JCE default on the system is used, which is usually 3DES.</p>
<p>Setting <tt>dfs.encrypt.data.transfer.cipher.suites</tt> to <tt>AES/CTR/NoPadding</tt> activates AES encryption. By default, this is unspecified, so AES is not used. When AES is used, the algorithm specified in <tt>dfs.encrypt.data.transfer.algorithm</tt> is still used during an initial key exchange. The AES key bit length can be configured by setting <tt>dfs.encrypt.data.transfer.cipher.key.bitlength</tt> to 128, 192 or 256. The default is 128.</p>
<p>AES offers the greatest cryptographic strength and the best performance. At this time, 3DES and RC4 have been used more often in Hadoop clusters.</p></div>
<div class="section">
<h3><a name="Data_Encryption_on_HTTP"></a>Data Encryption on HTTP</h3>
<p>Data transfer between Web-console and clients are protected by using SSL(HTTPS). SSL configuration is recommended but not required to configure Hadoop security with Kerberos.</p>
<p>To enable SSL for web console of HDFS daemons, set <tt>dfs.http.policy</tt> to either <tt>HTTPS_ONLY</tt> or <tt>HTTP_AND_HTTPS</tt> in hdfs-site.xml. Note KMS and HttpFS do not respect this parameter. See <a href="../../hadoop-kms/index.html">Hadoop KMS</a> and <a href="../../hadoop-hdfs-httpfs/ServerSetup.html">Hadoop HDFS over HTTP - Server Setup</a> for instructions on enabling KMS over HTTPS and HttpFS over HTTPS, respectively.</p>
<p>To enable SSL for web console of YARN daemons, set <tt>yarn.http.policy</tt> to <tt>HTTPS_ONLY</tt> in yarn-site.xml.</p>
<p>To enable SSL for web console of MapReduce JobHistory server, set <tt>mapreduce.jobhistory.http.policy</tt> to <tt>HTTPS_ONLY</tt> in mapred-site.xml.</p></div></div>
<div class="section">
<h2><a name="Configuration"></a>Configuration</h2>
<div class="section">
<h3><a name="Permissions_for_both_HDFS_and_local_fileSystem_paths"></a>Permissions for both HDFS and local fileSystem paths</h3>
<p>The following table lists various paths on HDFS and local filesystems (on all nodes) and recommended permissions:</p>
<table border="0" class="bodyTable">
<thead>

<tr class="a">
<th align="left"> Filesystem </th>
<th align="left"> Path                                         </th>
<th align="left"> User:Group    </th>
<th align="left"> Permissions   </th></tr>
</thead><tbody>

<tr class="b">
<td align="left"> local      </td>
<td align="left"> <tt>dfs.namenode.name.dir</tt>                      </td>
<td align="left"> hdfs:hadoop   </td>
<td align="left"> <tt>drwx------</tt>  </td></tr>
<tr class="a">
<td align="left"> local      </td>
<td align="left"> <tt>dfs.datanode.data.dir</tt>                      </td>
<td align="left"> hdfs:hadoop   </td>
<td align="left"> <tt>drwx------</tt>  </td></tr>
<tr class="b">
<td align="left"> local      </td>
<td align="left"> <tt>$HADOOP_LOG_DIR</tt>                            </td>
<td align="left"> hdfs:hadoop   </td>
<td align="left"> <tt>drwxrwxr-x</tt>  </td></tr>
<tr class="a">
<td align="left"> local      </td>
<td align="left"> <tt>$YARN_LOG_DIR</tt>                              </td>
<td align="left"> yarn:hadoop   </td>
<td align="left"> <tt>drwxrwxr-x</tt>  </td></tr>
<tr class="b">
<td align="left"> local      </td>
<td align="left"> <tt>yarn.nodemanager.local-dirs</tt>                </td>
<td align="left"> yarn:hadoop   </td>
<td align="left"> <tt>drwxr-xr-x</tt>  </td></tr>
<tr class="a">
<td align="left"> local      </td>
<td align="left"> <tt>yarn.nodemanager.log-dirs</tt>                  </td>
<td align="left"> yarn:hadoop   </td>
<td align="left"> <tt>drwxr-xr-x</tt>  </td></tr>
<tr class="b">
<td align="left"> local      </td>
<td align="left"> container-executor                           </td>
<td align="left"> root:hadoop   </td>
<td align="left"> <tt>--Sr-s--*</tt>   </td></tr>
<tr class="a">
<td align="left"> local      </td>
<td align="left"> <tt>conf/container-executor.cfg</tt>                </td>
<td align="left"> root:hadoop   </td>
<td align="left"> <tt>r-------*</tt>   </td></tr>
<tr class="b">
<td align="left"> hdfs       </td>
<td align="left"> <tt>/</tt>                                          </td>
<td align="left"> hdfs:hadoop   </td>
<td align="left"> <tt>drwxr-xr-x</tt>  </td></tr>
<tr class="a">
<td align="left"> hdfs       </td>
<td align="left"> <tt>/tmp</tt>                                       </td>
<td align="left"> hdfs:hadoop   </td>
<td align="left"> <tt>drwxrwxrwxt</tt> </td></tr>
<tr class="b">
<td align="left"> hdfs       </td>
<td align="left"> <tt>/user</tt>                                      </td>
<td align="left"> hdfs:hadoop   </td>
<td align="left"> <tt>drwxr-xr-x</tt>  </td></tr>
<tr class="a">
<td align="left"> hdfs       </td>
<td align="left"> <tt>yarn.nodemanager.remote-app-log-dir</tt>        </td>
<td align="left"> yarn:hadoop   </td>
<td align="left"> <tt>drwxrwxrwxt</tt> </td></tr>
<tr class="b">
<td align="left"> hdfs       </td>
<td align="left"> <tt>mapreduce.jobhistory.intermediate-done-dir</tt> </td>
<td align="left"> mapred:hadoop </td>
<td align="left"> <tt>drwxrwxrwxt</tt> </td></tr>
<tr class="a">
<td align="left"> hdfs       </td>
<td align="left"> <tt>mapreduce.jobhistory.done-dir</tt>              </td>
<td align="left"> mapred:hadoop </td>
<td align="left"> <tt>drwxr-x---</tt>  </td></tr>
</tbody>
</table></div>
<div class="section">
<h3><a name="Common_Configurations"></a>Common Configurations</h3>
<p>In order to turn on RPC authentication in hadoop, set the value of <tt>hadoop.security.authentication</tt> property to <tt>&quot;kerberos&quot;</tt>, and set security related settings listed below appropriately.</p>
<p>The following properties should be in the <tt>core-site.xml</tt> of all the nodes in the cluster.</p>
<table border="0" class="bodyTable">
<thead>

<tr class="a">
<th align="left"> Parameter                               </th>
<th align="left"> Value                                           </th>
<th align="left"> Notes                                                                                                                                                                               </th></tr>
</thead><tbody>

<tr class="b">
<td align="left"> <tt>hadoop.security.authentication</tt>        </td>
<td align="left"> <tt>kerberos</tt>                                      </td>
<td align="left"> <tt>simple</tt> : No authentication. (default) &#xa0;<tt>kerberos</tt> : Enable authentication by Kerberos.                                                                                            </td></tr>
<tr class="a">
<td align="left"> <tt>hadoop.security.authorization</tt>         </td>
<td align="left"> <tt>true</tt>                                          </td>
<td align="left"> Enable <a href="./ServiceLevelAuth.html">RPC service-level authorization</a>.                                                                                                                  </td></tr>
<tr class="b">
<td align="left"> <tt>hadoop.rpc.protection</tt>                 </td>
<td align="left"> <tt>authentication</tt>                                </td>
<td align="left"> <tt>authentication</tt> : authentication only (default); <tt>integrity</tt> : integrity check in addition to authentication;&#xa0;<tt>privacy</tt> : data encryption in addition to integrity                 </td></tr>
<tr class="a">
<td align="left"> <tt>hadoop.security.auth_to_local</tt>         </td>
<td align="left"> <tt>RULE:</tt><i><tt>exp1</tt></i>&#xa0;<tt>RULE:</tt><i><tt>exp2</tt></i>&#xa0;<i>&#x2026;</i>&#xa0;<tt>DEFAULT</tt> </td>
<td align="left"> The value is string containing new line characters. See <a class="externalLink" href="http://web.mit.edu/Kerberos/krb5-latest/doc/admin/conf_files/krb5_conf.html">Kerberos documentation</a> for the format of <i>exp</i>. </td></tr>
<tr class="b">
<td align="left"> <tt>hadoop.proxyuser.</tt><i>superuser</i><tt>.hosts</tt>  </td>
<td align="left">                                                 </td>
<td align="left"> comma separated hosts from which <i>superuser</i> access are allowed to impersonation. <tt>*</tt> means wildcard.                                                                               </td></tr>
<tr class="a">
<td align="left"> <tt>hadoop.proxyuser.</tt><i>superuser</i><tt>.groups</tt> </td>
<td align="left">                                                 </td>
<td align="left"> comma separated groups to which users impersonated by <i>superuser</i> belong. <tt>*</tt> means wildcard.                                                                                       </td></tr>
</tbody>
</table></div>
<div class="section">
<h3><a name="NameNode"></a>NameNode</h3>
<table border="0" class="bodyTable">
<thead>

<tr class="a">
<th align="left"> Parameter                                         </th>
<th align="left"> Value                                        </th>
<th align="left"> Notes                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </th></tr>
</thead><tbody>

<tr class="b">
<td align="left"> <tt>dfs.block.access.token.enable</tt>                   </td>
<td align="left"> <tt>true</tt>                                       </td>
<td align="left"> Enable HDFS block access tokens for secure operations.                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr>
<tr class="a">
<td align="left"> <tt>dfs.namenode.kerberos.principal</tt>                 </td>
<td align="left"> <tt>nn/_HOST@REALM.TLD</tt>                         </td>
<td align="left"> Kerberos principal name for the NameNode.                                                                                                                                                                                                                                                                                                                                                                                                                              </td></tr>
<tr class="b">
<td align="left"> <tt>dfs.namenode.keytab.file</tt>                        </td>
<td align="left"> <tt>/etc/security/keytab/nn.service.keytab</tt>     </td>
<td align="left"> Kerberos keytab file for the NameNode.                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr>
<tr class="a">
<td align="left"> <tt>dfs.namenode.kerberos.internal.spnego.principal</tt> </td>
<td align="left"> <tt>HTTP/_HOST@REALM.TLD</tt>                       </td>
<td align="left"> The server principal used by the NameNode for web UI SPNEGO authentication. The SPNEGO server principal begins with the prefix <tt>HTTP/</tt> by convention. If the value is <tt>'*'</tt>, the web server will attempt to login with every principal specified in the keytab file <tt>dfs.web.authentication.kerberos.keytab</tt>. For most deployments this can be set to <tt>${dfs.web.authentication.kerberos.principal}</tt> i.e use the value of <tt>dfs.web.authentication.kerberos.principal</tt>. </td></tr>
<tr class="b">
<td align="left"> <tt>dfs.web.authentication.kerberos.keytab</tt>          </td>
<td align="left"> <tt>/etc/security/keytab/spnego.service.keytab</tt> </td>
<td align="left"> SPNEGO keytab file for the NameNode. In HA clusters this setting is shared with the Journal Nodes.                                                                                                                                                                                                                                                                                                                                                                     </td></tr>
</tbody>
</table>
<p>The following settings allow configuring SSL access to the NameNode web UI (optional).</p>
<table border="0" class="bodyTable">
<thead>

<tr class="a">
<th align="left"> Parameter                    </th>
<th align="left"> Value                                           </th>
<th align="left"> Notes                                                                                                                                                                                                                                                                                                                                                                                              </th></tr>
</thead><tbody>

<tr class="b">
<td align="left"> <tt>dfs.http.policy</tt>            </td>
<td align="left"> <tt>HTTP_ONLY</tt> or <tt>HTTPS_ONLY</tt> or <tt>HTTP_AND_HTTPS</tt> </td>
<td align="left"> <tt>HTTPS_ONLY</tt> turns off http access. This option takes precedence over the deprecated configuration dfs.https.enable and hadoop.ssl.enabled. If using SASL to authenticate data transfer protocol instead of running DataNode as root and using privileged ports, then this property must be set to <tt>HTTPS_ONLY</tt> to guarantee authentication of HTTP servers. (See <tt>dfs.data.transfer.protection</tt>.) </td></tr>
<tr class="a">
<td align="left"> <tt>dfs.namenode.https-address</tt> </td>
<td align="left"> <tt>0.0.0.0:9871</tt>                                 </td>
<td align="left"> This parameter is used in non-HA mode and without federation. See <a href="../hadoop-hdfs/HDFSHighAvailabilityWithNFS.html#Deployment">HDFS High Availability</a> and <a href="../hadoop-hdfs/Federation.html#Federation_Configuration">HDFS Federation</a> for details.                                                                                                                                                 </td></tr>
<tr class="b">
<td align="left"> <tt>dfs.https.enable</tt>           </td>
<td align="left"> <tt>true</tt>                                          </td>
<td align="left"> This value is deprecated. <tt>Use dfs.http.policy</tt>                                                                                                                                                                                                                                                                                                                                                    </td></tr>
</tbody>
</table></div>
<div class="section">
<h3><a name="Secondary_NameNode"></a>Secondary NameNode</h3>
<table border="0" class="bodyTable">
<thead>

<tr class="a">
<th align="left"> Parameter                                                   </th>
<th align="left"> Value                                    </th>
<th align="left"> Notes                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </th></tr>
</thead><tbody>

<tr class="b">
<td align="left"> <tt>dfs.namenode.secondary.http-address</tt>                       </td>
<td align="left"> <tt>0.0.0.0:9868</tt>                          </td>
<td align="left"> HTTP web UI address for the Secondary NameNode.                                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr>
<tr class="a">
<td align="left"> <tt>dfs.namenode.secondary.https-address</tt>                      </td>
<td align="left"> <tt>0.0.0.0:9869</tt>                          </td>
<td align="left"> HTTPS web UI address for the Secondary NameNode.                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr>
<tr class="b">
<td align="left"> <tt>dfs.secondary.namenode.keytab.file</tt>                        </td>
<td align="left"> <tt>/etc/security/keytab/sn.service.keytab</tt> </td>
<td align="left"> Kerberos keytab file for the Secondary NameNode.                                                                                                                