<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--
 | Generated by Apache Maven Doxia at 2020-07-06
 | Rendered using Apache Maven Stylus Skin 1.5
-->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Apache Hadoop 3.3.0 &#x2013; MapReduce Tutorial</title>
    <style type="text/css" media="all">
      @import url("./css/maven-base.css");
      @import url("./css/maven-theme.css");
      @import url("./css/site.css");
    </style>
    <link rel="stylesheet" href="./css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20200706" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
                </head>
  <body class="composite">
    <div id="banner">
                        <a href="http://hadoop.apache.org/" id="bannerLeft">
                                        <img src="http://hadoop.apache.org/images/hadoop-logo.jpg" alt="" />
                </a>
                              <a href="http://www.apache.org/" id="bannerRight">
                                        <img src="http://www.apache.org/images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                   <div class="xleft">
                          <a href="http://www.apache.org/" class="externalLink">Apache</a>
        &gt;
                  <a href="http://hadoop.apache.org/" class="externalLink">Hadoop</a>
        &gt;
                  <a href="../index.html">Apache Hadoop MapReduce Client</a>
        &gt;
                  <a href="index.html">Apache Hadoop 3.3.0</a>
        &gt;
        MapReduce Tutorial
        </div>
            <div class="xright">            <a href="http://wiki.apache.org/hadoop" class="externalLink">Wiki</a>
            |
                <a href="https://gitbox.apache.org/repos/asf/hadoop.git" class="externalLink">git</a>
            |
                <a href="http://hadoop.apache.org/" class="externalLink">Apache Hadoop</a>
              
                                   &nbsp;| Last Published: 2020-07-06
              &nbsp;| Version: 3.3.0
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                   <h5>General</h5>
                  <ul>
                  <li class="none">
                  <a href="../../index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CommandsManual.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/FileSystemShell.html">FileSystem Shell</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Compatibility.html">Compatibility Specification</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/DownstreamDev.html">Downstream Developer's Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/AdminCompatibilityGuide.html">Admin Compatibility Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/InterfaceClassification.html">Interface Classification</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/filesystem/index.html">FileSystem Specification</a>
            </li>
          </ul>
                       <h5>Common</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html">CLI Mini Cluster</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/FairCallQueue.html">Fair Call Queue</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Superusers.html">Proxy User</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/RackAwareness.html">Rack Awareness</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/SecureMode.html">Secure Mode</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/ServiceLevelAuth.html">Service Level Authorization</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/HttpAuthentication.html">HTTP Authentication</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CredentialProviderAPI.html">Credential Provider API</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-kms/index.html">Hadoop KMS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Tracing.html">Tracing</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/UnixShellGuide.html">Unix Shell Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/registry/index.html">Registry</a>
            </li>
          </ul>
                       <h5>HDFS</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">User Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">NameNode HA With QJM</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">NameNode HA With NFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html">Observer NameNode</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/Federation.html">Federation</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ViewFs.html">ViewFs</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">Snapshots</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html">Edits Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html">Image Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">Permissions and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html">Quotas and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/LibHdfs.html">libhdfs (C API)</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS (REST API)</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-hdfs-httpfs/index.html">HttpFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">Short Circuit Local Reads</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">NFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">Rolling Upgrade</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html">Extended Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">Transparent Encryption</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html">Multihoming</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Storage Policies</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/MemoryStorage.html">Memory Storage Support</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/SLGUserGuide.html">Synthetic Load Generator</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html">Erasure Coding</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html">Disk Balancer</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsUpgradeDomain.html">Upgrade Domain</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsDataNodeAdminGuide.html">DataNode Admin</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html">Router Federation</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsProvidedStorage.html">Provided Storage</a>
            </li>
          </ul>
                       <h5>MapReduce</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Tutorial</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">Compatibility with 1.x</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html">Encrypted Shuffle</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html">Pluggable Shuffle/Sort</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html">Distributed Cache Deploy</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/SharedCacheSupport.html">Support for YARN Shared Cache</a>
            </li>
          </ul>
                       <h5>MapReduce REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">MR History Server</a>
            </li>
          </ul>
                       <h5>YARN</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YARN.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YarnCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/FairScheduler.html">Fair Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html">ResourceManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager HA</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceModel.html">Resource Model</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeLabel.html">Node Labels</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeAttributes.html">Node Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html">Timeline Service V.2</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html">YARN Application Security</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeManager.html">NodeManager</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/DockerContainers.html">Running Applications in Docker Containers</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/RuncContainers.html">Running Applications in runC Containers</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html">Using CGroups</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/SecureContainer.html">Secure Containers</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ReservationSystem.html">Reservation System</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/GracefulDecommission.html">Graceful Decommission</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/OpportunisticContainers.html">Opportunistic Containers</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/Federation.html">YARN Federation</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/SharedCache.html">Shared Cache</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/UsingGpus.html">Using GPU</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/UsingFPGA.html">Using FPGA</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/PlacementConstraints.html">Placement Constraints</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YarnUI2.html">YARN UI2</a>
            </li>
          </ul>
                       <h5>YARN REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Introduction</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">Resource Manager</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">Node Manager</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html#Timeline_Server_REST_API_v1">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html#Timeline_Service_v.2_REST_API">Timeline Service V.2</a>
            </li>
          </ul>
                       <h5>YARN Service</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/Overview.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/QuickStart.html">QuickStart</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/Concepts.html">Concepts</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/YarnServiceAPI.html">Yarn Service API</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/ServiceDiscovery.html">Service Discovery</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/yarn-service/SystemServices.html">System Services</a>
            </li>
          </ul>
                       <h5>Hadoop Compatible File Systems</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-aliyun/tools/hadoop-aliyun/index.html">Aliyun OSS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-aws/tools/hadoop-aws/index.html">Amazon S3</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-azure/index.html">Azure Blob Storage</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-azure-datalake/index.html">Azure Data Lake Storage</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-openstack/index.html">OpenStack Swift</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-cos/cloud-storage/index.html">Tencent COS</a>
            </li>
          </ul>
                       <h5>Auth</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-auth/index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/Examples.html">Examples</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/Configuration.html">Configuration</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/BuildingIt.html">Building</a>
            </li>
          </ul>
                       <h5>Tools</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-streaming/HadoopStreaming.html">Hadoop Streaming</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-archives/HadoopArchives.html">Hadoop Archives</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-archive-logs/HadoopArchiveLogs.html">Hadoop Archive Logs</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-distcp/DistCp.html">DistCp</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-gridmix/GridMix.html">GridMix</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-rumen/Rumen.html">Rumen</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-resourceestimator/ResourceEstimator.html">Resource Estimator Service</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-sls/SchedulerLoadSimulator.html">Scheduler Load Simulator</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Benchmarking.html">Hadoop Benchmarking</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-dynamometer/Dynamometer.html">Dynamometer</a>
            </li>
          </ul>
                       <h5>Reference</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/release/">Changelog and Release Notes</a>
            </li>
                  <li class="none">
                  <a href="../../api/index.html">Java API docs</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/UnixShellAPI.html">Unix Shell API</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Metrics.html">Metrics</a>
            </li>
          </ul>
                       <h5>Configuration</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/core-default.xml">core-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs-rbf/hdfs-rbf-default.xml">hdfs-rbf-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/DeprecatedProperties.html">Deprecated Properties</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="./images/logos/maven-feather.png"/>
        </a>
                       
                               </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!---
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<h1>MapReduce Tutorial</h1>
<ul>
<li><a href="#Purpose">Purpose</a></li>
<li><a href="#Prerequisites">Prerequisites</a></li>
<li><a href="#Overview">Overview</a></li>
<li><a href="#Inputs_and_Outputs">Inputs and Outputs</a></li>
<li><a href="#Example:_WordCount_v1.0">Example: WordCount v1.0</a>
<ul>
<li><a href="#Source_Code">Source Code</a></li>
<li><a href="#Usage">Usage</a></li>
<li><a href="#Walk-through">Walk-through</a></li></ul></li>
<li><a href="#MapReduce_-_User_Interfaces">MapReduce - User Interfaces</a>
<ul>
<li><a href="#Payload">Payload</a>
<ul>
<li><a href="#Mapper">Mapper</a></li>
<li><a href="#Reducer">Reducer</a></li>
<li><a href="#Partitioner">Partitioner</a></li>
<li><a href="#Counter">Counter</a></li></ul></li>
<li><a href="#Job_Configuration">Job Configuration</a></li>
<li><a href="#Task_Execution_.26_Environment">Task Execution &amp; Environment</a>
<ul>
<li><a href="#Memory_Management">Memory Management</a></li>
<li><a href="#Map_Parameters">Map Parameters</a></li>
<li><a href="#Shuffle.2FReduce_Parameters">Shuffle/Reduce Parameters</a></li>
<li><a href="#Configured_Parameters">Configured Parameters</a></li>
<li><a href="#Task_Logs">Task Logs</a></li>
<li><a href="#Distributing_Libraries">Distributing Libraries</a></li></ul></li>
<li><a href="#Job_Submission_and_Monitoring">Job Submission and Monitoring</a>
<ul>
<li><a href="#Job_Control">Job Control</a></li></ul></li>
<li><a href="#Job_Input">Job Input</a>
<ul>
<li><a href="#InputSplit">InputSplit</a></li>
<li><a href="#RecordReader">RecordReader</a></li></ul></li>
<li><a href="#Job_Output">Job Output</a>
<ul>
<li><a href="#OutputCommitter">OutputCommitter</a></li>
<li><a href="#Task_Side-Effect_Files">Task Side-Effect Files</a></li>
<li><a href="#RecordWriter">RecordWriter</a></li></ul></li>
<li><a href="#Other_Useful_Features">Other Useful Features</a>
<ul>
<li><a href="#Submitting_Jobs_to_Queues">Submitting Jobs to Queues</a></li>
<li><a href="#Counters">Counters</a></li>
<li><a href="#DistributedCache">DistributedCache</a></li>
<li><a href="#Profiling">Profiling</a></li>
<li><a href="#Debugging">Debugging</a></li>
<li><a href="#Data_Compression">Data Compression</a></li>
<li><a href="#Skipping_Bad_Records">Skipping Bad Records</a></li></ul></li>
<li><a href="#Example:_WordCount_v2.0">Example: WordCount v2.0</a>
<ul>
<li><a href="#Source_Code">Source Code</a></li>
<li><a href="#Sample_Runs">Sample Runs</a></li>
<li><a href="#Highlights">Highlights</a></li></ul></li></ul></li></ul>

<div class="section">
<h2><a name="Purpose"></a>Purpose</h2>
<p>This document comprehensively describes all user-facing facets of the Hadoop MapReduce framework and serves as a tutorial.</p></div>
<div class="section">
<h2><a name="Prerequisites"></a>Prerequisites</h2>
<p>Ensure that Hadoop is installed, configured and is running. More details:</p>
<ul>

<li>

<p><a href="../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a> for first-time users.</p>
</li>
<li>

<p><a href="../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a> for large, distributed clusters.</p>
</li>
</ul></div>
<div class="section">
<h2><a name="Overview"></a>Overview</h2>
<p>Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.</p>
<p>A MapReduce <i>job</i> usually splits the input data-set into independent chunks which are processed by the <i>map tasks</i> in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the <i>reduce tasks</i>. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks.</p>
<p>Typically the compute nodes and the storage nodes are the same, that is, the MapReduce framework and the Hadoop Distributed File System (see <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">HDFS Architecture Guide</a>) are running on the same set of nodes. This configuration allows the framework to effectively schedule tasks on the nodes where data is already present, resulting in very high aggregate bandwidth across the cluster.</p>
<p>The MapReduce framework consists of a single master <tt>ResourceManager</tt>, one worker <tt>NodeManager</tt> per cluster-node, and <tt>MRAppMaster</tt> per application (see <a href="../../hadoop-yarn/hadoop-yarn-site/YARN.html">YARN Architecture Guide</a>).</p>
<p>Minimally, applications specify the input/output locations and supply <i>map</i> and <i>reduce</i> functions via implementations of appropriate interfaces and/or abstract-classes. These, and other job parameters, comprise the <i>job configuration</i>.</p>
<p>The Hadoop <i>job client</i> then submits the job (jar/executable etc.) and configuration to the <tt>ResourceManager</tt> which then assumes the responsibility of distributing the software/configuration to the workers, scheduling tasks and monitoring them, providing status and diagnostic information to the job-client.</p>
<p>Although the Hadoop framework is implemented in Java&#x2122;, MapReduce applications need not be written in Java.</p>
<ul>

<li>

<p><a href="../../api/org/apache/hadoop/streaming/package-summary.html">Hadoop Streaming</a> is a utility which allows users to create and run jobs with any executables (e.g. shell utilities) as the mapper and/or the reducer.</p>
</li>
<li>

<p><a href="../../api/org/apache/hadoop/mapred/pipes/package-summary.html">Hadoop Pipes</a> is a <a class="externalLink" href="http://www.swig.org/">SWIG</a>-compatible C++ API to implement MapReduce applications (non JNI&#x2122; based).</p>
</li>
</ul></div>
<div class="section">
<h2><a name="Inputs_and_Outputs"></a>Inputs and Outputs</h2>
<p>The MapReduce framework operates exclusively on <tt>&lt;key, value&gt;</tt> pairs, that is, the framework views the input to the job as a set of <tt>&lt;key, value&gt;</tt> pairs and produces a set of <tt>&lt;key, value&gt;</tt> pairs as the output of the job, conceivably of different types.</p>
<p>The <tt>key</tt> and <tt>value</tt> classes have to be serializable by the framework and hence need to implement the <a href="../../api/org/apache/hadoop/io/Writable.html">Writable</a> interface. Additionally, the key classes have to implement the <a href="../../api/org/apache/hadoop/io/WritableComparable.html">WritableComparable</a> interface to facilitate sorting by the framework.</p>
<p>Input and Output types of a MapReduce job:</p>
<p>(input) <tt>&lt;k1, v1&gt; -&gt;</tt> <b>map</b> <tt>-&gt; &lt;k2, v2&gt; -&gt;</tt> <b>combine</b> <tt>-&gt; &lt;k2, v2&gt; -&gt;</tt> <b>reduce</b> <tt>-&gt; &lt;k3, v3&gt;</tt> (output)</p></div>
<div class="section">
<h2><a name="Example:_WordCount_v1.0"></a>Example: WordCount v1.0</h2>
<p>Before we jump into the details, lets walk through an example MapReduce application to get a flavour for how they work.</p>
<p><tt>WordCount</tt> is a simple application that counts the number of occurrences of each word in a given input set.</p>
<p>This works with a local-standalone, pseudo-distributed or fully-distributed Hadoop installation (<a href="../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>).</p>
<div class="section">
<h3><a name="Source_Code"></a>Source Code</h3>

<div>
<div>
<pre class="source">import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper&lt;Object, Text, Text, IntWritable&gt;{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, &quot;word count&quot;);
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
</pre></div></div>
</div>
<div class="section">
<h3><a name="Usage"></a>Usage</h3>
<p>Assuming environment variables are set as follows:</p>

<div>
<div>
<pre class="source">export JAVA_HOME=/usr/java/default
export PATH=${JAVA_HOME}/bin:${PATH}
export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar
</pre></div></div>

<p>Compile <tt>WordCount.java</tt> and create a jar:</p>

<div>
<div>
<pre class="source">$ bin/hadoop com.sun.tools.javac.Main WordCount.java
$ jar cf wc.jar WordCount*.class
</pre></div></div>

<p>Assuming that:</p>
<ul>

<li><tt>/user/joe/wordcount/input</tt> - input directory in HDFS</li>
<li><tt>/user/joe/wordcount/output</tt> - output directory in HDFS</li>
</ul>
<p>Sample text-files as input:</p>

<div>
<div>
<pre class="source">$ bin/hadoop fs -ls /user/joe/wordcount/input/
/user/joe/wordcount/input/file01
/user/joe/wordcount/input/file02

$ bin/hadoop fs -cat /user/joe/wordcount/input/file01
Hello World Bye World

$ bin/hadoop fs -cat /user/joe/wordcount/input/file02
Hello Hadoop Goodbye Hadoop
</pre></div></div>

<p>Run the application:</p>

<div>
<div>
<pre class="source">$ bin/hadoop jar wc.jar WordCount /user/joe/wordcount/input /user/joe/wordcount/output
</pre></div></div>

<p>Output:</p>

<div>
<div>
<pre class="source">$ bin/hadoop fs -cat /user/joe/wordcount/output/part-r-00000
Bye 1
Goodbye 1
Hadoop 2
Hello 2
World 2
</pre></div></div>

<p>Applications can specify a comma separated list of paths which would be present in the current working directory of the task using the option <tt>-files</tt>. The <tt>-libjars</tt> option allows applications to add jars to the classpaths of the maps and reduces. The option <tt>-archives</tt> allows them to pass comma separated list of archives as arguments. These archives are unarchived and a link with name of the archive is created in the current working directory of tasks. More details about the command line options are available at <a href="../../hadoop-project-dist/hadoop-common/CommandsManual.html">Commands Guide</a>.</p>
<p>Running <tt>wordcount</tt> example with <tt>-libjars</tt>, <tt>-files</tt> and <tt>-archives</tt>:</p>

<div>
<div>
<pre class="source">bin/hadoop jar hadoop-mapreduce-examples-&lt;ver&gt;.jar wordcount -files cachefile.txt -libjars mylib.jar -archives myarchive.zip input output
</pre></div></div>

<p>Here, myarchive.zip will be placed and unzipped into a directory by the name &#x201c;myarchive.zip&#x201d;.</p>
<p>Users can specify a different symbolic name for files and archives passed through <tt>-files</tt> and <tt>-archives</tt> option, using #.</p>
<p>For example,</p>

<div>
<div>
<pre class="source">bin/hadoop jar hadoop-mapreduce-examples-&lt;ver&gt;.jar wordcount -files dir1/dict.txt#dict1,dir2/dict.txt#dict2 -archives mytar.tgz#tgzdir input output
</pre></div></div>

<p>Here, the files dir1/dict.txt and dir2/dict.txt can be accessed by tasks using the symbolic names dict1 and dict2 respectively. The archive mytar.tgz will be placed and unarchived into a directory by the name &#x201c;tgzdir&#x201d;.</p>
<p>Applications can specify environment variables for mapper, reducer, and application master tasks by specifying them on the command line using the options -Dmapreduce.map.env, -Dmapreduce.reduce.env, and -Dyarn.app.mapreduce.am.env, respectively.</p>
<p>For example the following sets environment variables FOO_VAR=bar and LIST_VAR=a,b,c for the mappers and reducers,</p>

<div>
<div>
<pre class="source">bin/hadoop jar hadoop-mapreduce-examples-&lt;ver&gt;.jar wordcount -Dmapreduce.map.env.FOO_VAR=bar -Dmapreduce.map.env.LIST_VAR=a,b,c -Dmapreduce.reduce.env.FOO_VAR=bar -Dmapreduce.reduce.env.LIST_VAR=a,b,c input output
</pre></div></div>
</div>
<div class="section">
<h3><a name="Walk-through"></a>Walk-through</h3>
<p>The <tt>WordCount</tt> application is quite straight-forward.</p>

<div>
<div>
<pre class="source">public void map(Object key, Text value, Context context
                ) throws IOException, InterruptedException {
  StringTokenizer itr = new StringTokenizer(value.toString());
  while (itr.hasMoreTokens()) {
    word.set(itr.nextToken());
    context.write(word, one);
  }
}
</pre></div></div>

<p>The <tt>Mapper</tt> implementation, via the <tt>map</tt> method, processes one line at a time, as provided by the specified <tt>TextInputFormat</tt>. It then splits the line into tokens separated by whitespaces, via the <tt>StringTokenizer</tt>, and emits a key-value pair of <tt>&lt; &lt;word&gt;, 1&gt;</tt>.</p>
<p>For the given sample input the first map emits:</p>

<div>
<div>
<pre class="source">&lt; Hello, 1&gt;
&lt; World, 1&gt;
&lt; Bye, 1&gt;
&lt; World, 1&gt;
</pre></div></div>

<p>The second map emits:</p>

<div>
<div>
<pre class="source">&lt; Hello, 1&gt;
&lt; Hadoop, 1&gt;
&lt; Goodbye, 1&gt;
&lt; Hadoop, 1&gt;
</pre></div></div>

<p>We&#x2019;ll learn more about the number of maps spawned for a given job, and how to control them in a fine-grained manner, a bit later in the tutorial.</p>

<div>
<div>
<pre class="source">    job.setCombinerClass(IntSumReducer.class);
</pre></div></div>

<p><tt>WordCount</tt> also specifies a <tt>combiner</tt>. Hence, the output of each map is passed through the local combiner (which is same as the <tt>Reducer</tt> as per the job configuration) for local aggregation, after being sorted on the <i>key</i>s.</p>
<p>The output of the first map:</p>

<div>
<div>
<pre class="source">&lt; Bye, 1&gt;
&lt; Hello, 1&gt;
&lt; World, 2&gt;
</pre></div></div>

<p>The output of the second map:</p>

<div>
<div>
<pre class="source">&lt; Goodbye, 1&gt;
&lt; Hadoop, 2&gt;
&lt; Hello, 1&gt;
</pre></div></div>

<div>
<div>
<pre class="source">public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                   Context context
                   ) throws IOException, InterruptedException {
  int sum = 0;
  for (IntWritable val : values) {
    sum += val.get();
  }
  result.set(sum);
  context.write(key, result);
}
</pre></div></div>

<p>The <tt>Reducer</tt> implementation, via the <tt>reduce</tt> method just sums up the values, which are the occurrence counts for each key (i.e. words in this example).</p>
<p>Thus the output of the job is:</p>

<div>
<div>
<pre class="source">&lt; Bye, 1&gt;
&lt; Goodbye, 1&gt;
&lt; Hadoop, 2&gt;
&lt; Hello, 2&gt;
&lt; World, 2&gt;
</pre></div></div>

<p>The <tt>main</tt> method specifies various facets of the job, such as the input/output paths (passed via the command line), key/value types, input/output formats etc., in the <tt>Job</tt>. It then calls the <tt>job.waitForCompletion</tt> to submit the job and monitor its progress.</p>
<p>We&#x2019;ll learn more about <tt>Job</tt>, <tt>InputFormat</tt>, <tt>OutputFormat</tt> and other interfaces and classes a bit later in the tutorial.</p></div></div>
<div class="section">
<h2><a name="MapReduce_-_User_Interfaces"></a>MapReduce - User Interfaces</h2>
<p>This section provides a reasonable amount of detail on every user-facing aspect of the MapReduce framework. This should help users implement, configure and tune their jobs in a fine-grained manner. However, please note that the javadoc for each class/interface remains the most comprehensive documentation available; this is only meant to be a tutorial.</p>
<p>Let us first take the <tt>Mapper</tt> and <tt>Reducer</tt> interfaces. Applications typically implement them to provide the <tt>map</tt> and <tt>reduce</tt> methods.</p>
<p>We will then discuss other core interfaces including <tt>Job</tt>, <tt>Partitioner</tt>, <tt>InputFormat</tt>, <tt>OutputFormat</tt>, and others.</p>
<p>Finally, we will wrap up by discussing some useful features of the framework such as the <tt>DistributedCache</tt>, <tt>IsolationRunner</tt> etc.</p>
<div class="section">
<h3><a name="Payload"></a>Payload</h3>
<p>Applications typically implement the <tt>Mapper</tt> and <tt>Reducer</tt> interfaces to provide the <tt>map</tt> and <tt>reduce</tt> methods. These form the core of the job.</p>
<div class="section">
<h4><a name="Mapper"></a>Mapper</h4>
<p><a href="../../api/org/apache/hadoop/mapreduce/Mapper.html">Mapper</a> maps input key/value pairs to a set of intermediate key/value pairs.</p>
<p>Maps are the individual tasks that transform input records into intermediate records. The transformed intermediate records do not need to be of the same type as the input records. A given input pair may map to zero or many output pairs.</p>
<p>The Hadoop MapReduce framework spawns one map task for each <tt>InputSplit</tt> generated by the <tt>InputFormat</tt> for the job.</p>
<p>Overall, mapper implementations are passed to the job via <a href="../../api/org/apache/hadoop/mapreduce/Job.html">Job.setMapperClass(Class)</a> method. The framework then calls <a href="../../api/org/apache/hadoop/mapreduce/Mapper.html">map(WritableComparable, Writable, Context)</a> for each key/value pair in the <tt>InputSplit</tt> for that task. Applications can then override the <tt>cleanup(Context)</tt> method to perform any required cleanup.</p>
<p>Output pairs do not need to be of the same types as input pairs. A given input pair may map to zero or many output pairs. Output pairs are collected with calls to context.write(WritableComparable, Writable).</p>
<p>Applications can use the <tt>Counter</tt> to report its statistics.</p>
<p>All intermediate values associated with a given output key are subsequently grouped by the framework, and passed to the <tt>Reducer</tt>(s) to determine the final output. Users can control the grouping by specifying a <tt>Comparator</tt> via <a href="../../api/org/apache/hadoop/mapreduce/Job.html">Job.setGroupingComparatorClass(Class)</a>.</p>
<p>The <tt>Mapper</tt> outputs are sorted and then partitioned per <tt>Reducer</tt>. The total number of partitions is the same as the number of reduce tasks for the job. Users can control which keys (and hence records) go to which <tt>Reducer</tt> by implementing a custom <tt>Partitioner</tt>.</p>
<p>Users can optionally specify a <tt>combiner</tt>, via <a href="../../api/org/apache/hadoop/mapreduce/Job.html">Job.setCombinerClass(Class)</a>, to perform local aggregation of the intermediate outputs, which helps to cut down the amount of data transferred from the <tt>Mapper</tt> to the <tt>Reducer</tt>.</p>
<p>The intermediate, sorted outputs are always stored in a simple (key-len, key, value-len, value) format. Applications can control if, and how, the intermediate outputs are to be compressed and the <a href="../../api/org/apache/hadoop/io/compress/CompressionCodec.html">CompressionCodec</a> to be used via the <tt>Configuration</tt>.</p>
<div class="section">
<h5><a name="How_Many_Maps.3F"></a>How Many Maps?</h5>
<p>The number of maps is usually driven by the total size of the inputs, that is, the total number of blocks of the input files.</p>
<p>The right level of parallelism for maps seems to be around 10-100 maps per-node, although it has been set up to 300 maps for very cpu-light map tasks. Task setup takes a while, so it is best if the maps take at least a minute to execute.</p>
<p>Thus, if you expect 10TB of input data and have a blocksize of <tt>128MB</tt>, you&#x2019;ll end up with 82,000 maps, unless Configuration.set(<tt>MRJobConfig.NUM_MAPS</tt>, int) (which only provides a hint to the framework) is used to set it even higher.</p></div></div>
<div class="section">
<h4><a name="Reducer"></a>Reducer</h4>
<p><a href="../../api/org/apache/hadoop/mapreduce/Reducer.html">Reducer</a> reduces a set of intermediate values which share a key to a smaller set of values.</p>
<p>The number of reduces for the job is set by the user via <a href="../../api/org/apache/hadoop/mapreduce/Job.html">Job.setNumReduceTasks(int)</a>.</p>
<p>Overall, <tt>Reducer</tt> implementations are passed the <tt>Job</tt> for the job via the <a href="../../api/org/apache/hadoop/mapreduce/Job.html">Job.setReducerClass(Class)</a> method and can override it to initialize themselves. The framework then calls <a href="../../api/org/apache/hadoop/mapreduce/Reducer.html">reduce(WritableComparable, Iterable&lt;Writable&gt;, Context)</a> method for each <tt>&lt;key, (list of values)&gt;</tt> pair in the grouped inputs. Applications can then override the <tt>cleanup(Context)</tt> method to perform any required cleanup.</p>
<p><tt>Reducer</tt> has 3 primary phases: shuffle, sort and reduce.</p>
<div class="section">
<h5><a name="Shuffle"></a>Shuffle</h5>
<p>Input to the <tt>Reducer</tt> is the sorted output of the mappers. In this phase the framework fetches the relevant partition of the output of all the mappers, via HTTP.</p></div>
<div class="section">
<h5><a name="Sort"></a>Sort</h5>
<p>The framework groups <tt>Reducer</tt> inputs by keys (since different mappers may have output the same key) in this stage.</p>
<p>The shuffle and sort phases occur simultaneously; while map-outputs are being fetched they are merged.</p></div>
<div class="section">
<h5><a name="Secondary_Sort"></a>Secondary Sort</h5>
<p>If equivalence rules for grouping the intermediate keys are required to be different from those for grouping keys before reduction, then one may specify a <tt>Comparator</tt> via <a href="../../api/org/apache/hadoop/mapreduce/Job.html">Job.setSortComparatorClass(Class)</a>. Since <a href="../../api/org/apache/hadoop/mapreduce/Job.html">Job.setGroupingComparatorClass(Class)</a> can be used to control how intermediate keys are grouped, these can be used in conjunction to simulate <i>secondary sort on values</i>.</p></div>
<div class="section">
<h5><a name="Reduce"></a>Reduce</h5>
<p>In this phase the reduce(WritableComparable, Iterable&lt;Writable&gt;, Context) method is called for each <tt>&lt;key, (list of values)&gt;</tt> pair in the grouped inputs.</p>
<p>The output of the reduce task is typically written to the <a href="../../api/org/apache/hadoop/fs/FileSystem.html">FileSystem</a> via Context.write(WritableComparable, Writable).</p>
<p>Applications can use the <tt>Counter</tt> to report its statistics.</p>
<p>The output of the <tt>Reducer</tt> is <i>not sorted</i>.</p></div>
<div class="section">
<h5><a name="How_Many_Reduces.3F"></a>How Many Reduces?</h5>
<p>The right number of reduces seems to be <tt>0.95</tt> or <tt>1.75</tt> multiplied by (&lt;<i>no. of nodes</i>&gt; * &lt;<i>no. of maximum containers per node</i>&gt;).</p>
<p>With <tt>0.95</tt> all of the reduces can launch immediately and start transferring map outputs as the maps finish. With <tt>1.75</tt> the faster nodes will finish their first round of reduces and launch a second wave of reduces doing a much better job of load balancing.</p>
<p>Increasing the number of reduces increases the framework overhead, but increases load balancing and lowers the cost of failures.</p>
<p>The scaling factors above are slightly less than whole numbers to reserve a few reduce slots in the framework for speculative-tasks and failed tasks.</p></div>
<div class="section">
<h5><a name="Reducer_NONE"></a>Reducer NONE</h5>
<p>It is legal to set the number of reduce-tasks to <i>zero</i> if no reduction is desired.</p>
<p>In this case the outputs of the map-tasks go directly to the <tt>FileSystem</tt>, into the output path set by <a href="../../api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.html">FileOutputFormat.setOutputPath(Job, Path)</a>. The framework does not sort the map-outputs before writing them out to the <tt>FileSystem</tt>.</p></div></div>
<div class="section">
<h4><a name="Partitioner"></a>Partitioner</h4>
<p><a href="../../api/org/apache/hadoop/mapreduce/Partitioner.html">Partitioner</a> partitions the key space.</p>
<p>Partitioner controls the partitioning of the keys of the intermediate map-outputs. The key (or a subset of the key) is used to derive the partition, typically by a <i>hash function</i>. The total number of partitions is the same as the number of reduce tasks for the job. Hence this controls which of the <tt>m</tt> reduce tasks the intermediate key (and hence the record) is sent to for reduction.</p>
<p><a href="../../api/org/apache/hadoop/mapreduce/lib/partition/HashPartitioner.html">HashPartitioner</a> is the default <tt>Partitioner</tt>.</p></div>
<div class="section">
<h4><a name="Counter"></a>Counter</h4>
<p><a href="../../api/org/apache/hadoop/mapreduce/Counter.html">Counter</a> is a facility for MapReduce applications to report its statistics.</p>
<p><tt>Mapper</tt> and <tt>Reducer</tt> implementations can use the <tt>Counter</tt> to report statistics.</p>
<p>Hadoop MapReduce comes bundled with a <a href="../../api/org/apache/hadoop/mapreduce/package-summary.html">library</a> of generally useful mappers, reducers, and partitioners.</p></div></div>
<div class="section">
<h3><a name="Job_Configuration"></a>Job Configuration</h3>
<p><a href="../../api/org/apache/hadoop/mapreduce/Job.html">Job</a> represents a MapReduce job configuration.</p>
<p><tt>Job</tt> is the primary interface for a user to describe a MapReduce job to the Hadoop framework for execution. The framework tries to faithfully execute the job as described by <tt>Job</tt>, however:</p>
<ul>

<li>

<p>Some configuration parameters may have been marked as final by administrators (see <a href="../../api/org/apache/hadoop/conf/Configuration.html#FinalParams">Final Parameters</a>) and hence cannot be altered.</p>
</li>
<li>

<p>While some job parameters are straight-forward to set (e.g. <a href="../../api/org/apache/hadoop/mapreduce/Job.html">Job.setNumReduceTasks(int)</a>) , other parameters interact subtly with the rest of the framework and/or job configuration and are more complex to set (e.g. <a href="../../api/org/apache/hadoop/conf/Configuration.html">Configuration.set(<tt>JobContext.NUM_MAPS</tt>, int)</a>).</p>
</li>
</ul>
<p><tt>Job</tt> is typically used to specify the <tt>Mapper</tt>, combiner (if any), <tt>Partitioner</tt>, <tt>Reducer</tt>, <tt>InputFormat</tt>, <tt>OutputFormat</tt> implementations. <a href="../../api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html">FileInputFormat</a> indicates the set of input files (<a href="../../api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html">FileInputFormat.setInputPaths(Job, Path&#x2026;)</a>/ <a href="../../api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html">FileInputFormat.addInputPath(Job, Path)</a>) and (<a href="../../api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html">FileInputFormat.setInputPaths(Job, String&#x2026;)</a>/ <a href="../../api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html">FileInputFormat.addInputPaths(Job, String))</a> and where the output files should be written (<a href="../../api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.html">FileOutputFormat.setOutputPath(Path)</a>).</p>
<p>Optionally, <tt>Job</tt> is used to specify other advanced facets of the job such as the <tt>Comparator</tt> to be used, files to be put in the <tt>DistributedCache</tt>, whether intermediate and/or job outputs are to be compressed (and how), whether job tasks can be executed in a <i>speculative</i> manner (<a href="../../api/org/apache/hadoop/mapreduce/Job.html">setMapSpeculativeExecution(boolean)</a>)/ <a href="../../api/org/apache/hadoop/mapreduce/Job.html">setReduceSpeculativeExecution(boolean)</a>), maximum number of attempts per task (<a href="../../api/org/apache/hadoop/mapreduce/Job.html">setMaxMapAttempts(int)</a>/ <a href="../../api/org/apache/hadoop/mapreduce/Job.html">setMaxReduceAttempts(int)</a>) etc.</p>
<p>Of course, users can use <a href="../../api/org/apache/hadoop/conf/Configuration.html">Configuration.set(String, String)</a>/ <a href="../../api/org/apache/hadoop/conf/Configuration.html">Configuration.get(String)</a> to set/get arbitrary parameters needed by applications. However, use the <tt>DistributedCache</tt> for large amounts of (read-only) data.</p></div>
<div class="section">
<h3><a name="Task_Execution_.26_Environment"></a>Task Execution &amp; Environment</h3>
<p>The <tt>MRAppMaster</tt> executes the <tt>Mapper</tt>/<tt>Reducer</tt> <i>task</i> as a child process in a separate jvm.</p>
<p>The child-task inherits the environment of the parent <tt>MRAppMaster</tt>. The user can specify additional options to the child-jvm via the <tt>mapreduce.{map|reduce}.java.opts</tt> and configuration parameter in the <tt>Job</tt> such as non-standard paths for the run-time linker to search shared libraries via <tt>-Djava.library.path=&lt;&gt;</tt> etc. If the <tt>mapreduce.{map|reduce}.java.opts</tt> parameters contains the symbol <i>@taskid@</i> it is interpolated with value of <tt>taskid</tt> of the MapReduce task.</p>
<p>Here is an example with multiple arguments and substitutions, showing jvm GC logging, and start of a passwordless JVM JMX agent so that it can connect with jconsole and the likes to watch child memory, threads and get thread dumps. It also sets the maximum heap-size of the map and reduce child jvm to 512MB &amp; 1024MB respectively. It also adds an additional path to the <tt>java.library.path</tt> of the child-jvm.</p>

<div>
<div>
<pre class="source">&lt;property&gt;
  &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;
  &lt;value&gt;
  -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc
  -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false
  &lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;
  &lt;value&gt;
  -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc
  -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false
  &lt;/value&gt;
&lt;/property&gt;
</pre></div></div>

<div class="section">
<h4><a name="Memory_Management"></a>Memory Management</h4>
<p>Users/admins can also specify the maximum virtual memory of the launched child-task, and any sub-process it launches recursively, using <tt>mapreduce.{map|reduce}.memory.mb</tt>. Note that the value set here is a per process limit. The value for <tt>mapreduce.{map|reduce}.memory.mb</tt> should be specified in mega bytes (MB). And also the value must be greater than or equal to the -Xmx passed to JavaVM, else the VM might not start.</p>
<p>Note: <tt>mapreduce.{map|reduce}.java.opts</tt> are used only for configuring the launched child tasks from MRAppMaster. Configuring the memory options for daemons is documented in <a href="../../hadoop-project-dist/hadoop-common/ClusterSetup.html#Configuring_Environment_of_Hadoop_Daemons">Configuring the Environment of the Hadoop Daemons</a>.</p>
<p>The memory available to some parts of the framework is also configurable. In map and reduce tasks, performance may be influenced by adjusting parameters influencing the concurrency of operations and the frequency with which data will hit disk. Monitoring the filesystem counters for a job- particularly relative to byte counts from the map and into the reduce- is invaluable to the tuning of these parameters.</p></div>
<div class="section">
<h4><a name="Map_Parameters"></a>Map Parameters</h4>
<p>A record emitted from a map will be serialized into a buffer and metadata will be stored into accounting buffers. As described in the following options, when either the serialization buffer or the metadata exceed a threshold, the contents of the buffers will be sorted and written to disk in the background while the map continues to output records. If either buffer fills completely while the spill is in progress, the map thread will block. When the map is finished, any remaining records are written to disk and all on-disk segments are merged into a single file. Minimizing the number of spills to disk can decrease map time, but a larger buffer also decreases the memory available to the mapper.</p>
<table border="0" class="bodyTable">
<thead>

<tr class="a">
<th align="left">               Name </th>
<th align="left">  Type </th>
<th align="left">                                                           Description </th></tr>
</thead><tbody>

<tr class="b">
<td align="left">     mapreduce.task.io.sort.mb </td>
<td align="left">  int </td>
<td align="left">       The cumulative size of the serialization and accounting buffers storing records emitted from the map, in megabytes. </td></tr>
<tr class="a">
<td align="left"> mapreduce.map.sort.spill.percent </td>
<td align="left"> float </td>
<td align="left"> The soft limit in the serialization buffer. Once reached, a thread will begin to spill the contents to disk in the background. </td></tr>
</tbody>
</table>
<p>Other notes</p>
<ul>

<li>

<p>If either spill threshold is exceeded while a spill is in progress, collection will continue until the spill is finished. For example, if <tt>mapreduce.map.sort.spill.percent</tt> is set to 0.33, and the remainder of the buffer is filled while the spill runs, the next spill will include all the collected records, or 0.66 of the buffer, and will not generate additional spills. In other words, the thresholds are defining triggers, not blocking.</p>
</li>
<li>

<p>A record larger than the serialization buffer will first trigger a spill, then be spilled to a separate file. It is undefined whether or not this record will first pass through the combiner.</p>
</li>
</ul></div>
<div class="section">
<h4><a name="Shuffle.2FReduce_Parameters"></a>Shuffle/Reduce Parameters</h4>
<p>As described previously, each reduce fetches the output assigned to it by the Partitioner via HTTP into memory and periodically merges these outputs to disk. If intermediate compression of map outputs is turned on, each output is decompressed into memory. The following options affect the frequency of these merges to disk prior to the reduce and the memory allocated to map output during the reduce.</p>
<table border="0" class="bodyTable">
<thead>

<tr class="a">
<th align="left"> Name </th>
<th align="left"> Type </th>
<th align="left"> Description </th></tr>
</thead><tbody>

<tr class="b">
<td align="left"> mapreduce.task.io.soft.factor </td>
<td align="left"> int </td>
<td align="left"> Specifies the number of segments on disk to be merged at the same time. It limits the number of open files and compression codecs during merge. If the number of files exceeds this limit, the merge will proceed in several passes. Though this limit also applies to the map, most jobs should be configured so that hitting this limit is unlikely there. </td></tr>
<tr class="a">
<td align="left"> mapreduce.reduce.merge.inmem.thresholds </td>
<td align="left"> int </td>
<td align="left"> The number of sorted map outputs fetched into memory before being merged to disk. Like the spill thresholds in the preceding note, this is not defining a unit of partition, but a trigger. In practice, this is usually set very high (1000) or disabled (0), since merging in-memory segments is often less expensive than merging from disk (see notes following this table). This threshold influences only the frequency of in-memory merges during the shuffle. </td></tr>
<tr class="b">
<td align="left"> mapreduce.reduce.shuffle.merge.percent </td>
<td align="left"> float </td>
<td align="left"> The memory threshold for fetched map outputs before an in-memory merge is started, expressed as a percentage of memory allocated to storing map outputs in memory. Since map outputs that can&#x2019;t fit in memory can be stalled, setting this high may decrease parallelism between the fetch and merge. Conversely, values as high as 1.0 have been effective for reduces whose input can fit entirely in memory. This parameter influences only the frequency of in-memory merges during the shuffle. </td></tr>
<tr class="a">
<td align="left"> mapreduce.reduce.shuffle.input.buffer.percent </td>
<td align="left"> float </td>
<td align="left"> The percentage of memory- relative to the maximum heapsize as typically specified in <tt>mapreduce.reduce.java.opts</tt>- that can be allocated to storing map outputs during the shuffle. Though some memory should be set aside for the framework, in general it is advantageous to set this high enough to store large and numerous map outputs. </td></tr>
<tr class="b">
<td align="left"> mapreduce.reduce.input.buffer.percent </td>
<td align="left"> float </td>
<td align="left"> The percentage of memory relative to the maximum heapsize in which map outputs may be retained during the reduce. When the reduce begins, map outputs will be merged to disk until those that remain are under the resource limit this defines. By default, all map outputs are merged to disk before the reduce begins to maximize the memory available to the reduce. For less memory-intensive reduces, this should be increased to avoid trips to disk. </td></tr>
</tbody>
</table>
<p>Other notes</p>
<ul>

<li>

<p>If a map output is larger than 25 percent of the memory allocated to copying map outputs, it will be written directly to disk without first staging through memory.</p>
</li>
<li>

<p>When running with a combiner, the reasoning about high merge thresholds and large buffers may not hold. For merges started before all map outputs have been fetched, the combiner is run while spilling to disk. In some cases, one can obtain better reduce times by spending resources combining map outputs- making disk spills small and parallelizing spilling and fetching- rather than aggressively increasing buffer sizes.</p>
</li>
<li>

<p>When merging in-memory map outputs to disk to begin the reduce, if an intermediate merge is necessary because there are segments to spill and at least <tt>mapreduce.task.io.sort.factor</tt> segments already on disk, the in-memory map outputs will be part of the intermediate merge.</p>
</li>
</ul></div>
<div class="section">
<h4><a name="Configured_Parameters"></a>Configured Parameters</h4>
<p>The following properties are localized in the job configuration for each task&#x2019;s execution:</p>
<table border="0" class="bodyTable">
<thead>

<tr class="a">
<th align="left"> Name </th>
<th align="left"> Type </th>
<th align="left"> Description </th></t